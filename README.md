# mlp-from-scratch
This project features a multilayer perceptron (MLP) built from scratch using only NumPy. This project implements backpropagation and gradient descent optimization to provide a foundational understanding of neural network mechanics.

## Implementation
1. **Manual Backpropagation:** Gradients are derived and implemented explicitly for each layer.
2. **Custom Optimization:** Model parameters are updated using first-principles gradient descent.
3. **Minimal Dependencies:** Uses NumPy only to maintain transparency and educational clarity.
4. **Modular Architecture:** Layers, activation functions, and loss functions are implemented as separate, reusable components.

## Objective
The primary objective of this project is to develop a concrete understanding of backpropagation at a mathematical level through implementation and experimentation. 
This includes analyzing training dynamics, tuning hyperparameters, and observing underfitting and overfitting behavior in practice.

## Status
Work in progress
